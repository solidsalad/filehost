{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f0aa7d77-b609-44d3-9586-2271e5610e38",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "warnings.filterwarnings(\"ignore\", module=\"threadpoolctl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "5f80fe89-dfb8-44ba-9506-7b5b47763ac1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://****@pkgs.dev.azure.com/umicore/DataAnalytics.RBM.Applications/_packaging/DataAnalytics.RBM.Packages/pypi/simple/\nRequirement already satisfied: opencv-python-headless in /local_disk0/.ephemeral_nfs/envs/pythonEnv-9b1d644c-02b5-47b7-853a-504f4a82fd02/lib/python3.11/site-packages (4.11.0.86)\nRequirement already satisfied: kaleido in /local_disk0/.ephemeral_nfs/envs/pythonEnv-9b1d644c-02b5-47b7-853a-504f4a82fd02/lib/python3.11/site-packages (0.2.1)\nRequirement already satisfied: open_clip_torch in /local_disk0/.ephemeral_nfs/envs/pythonEnv-9b1d644c-02b5-47b7-853a-504f4a82fd02/lib/python3.11/site-packages (2.32.0)\nRequirement already satisfied: pandas in /databricks/python3/lib/python3.11/site-packages (1.5.3)\nRequirement already satisfied: imgaug in /local_disk0/.ephemeral_nfs/envs/pythonEnv-9b1d644c-02b5-47b7-853a-504f4a82fd02/lib/python3.11/site-packages (0.4.0)\nRequirement already satisfied: numpy>=1.21.2 in /databricks/python3/lib/python3.11/site-packages (from opencv-python-headless) (1.23.5)\nRequirement already satisfied: torch>=1.9.0 in /databricks/python3/lib/python3.11/site-packages (from open_clip_torch) (2.3.1+cpu)\nRequirement already satisfied: torchvision in /databricks/python3/lib/python3.11/site-packages (from open_clip_torch) (0.18.1+cpu)\nRequirement already satisfied: regex in /databricks/python3/lib/python3.11/site-packages (from open_clip_torch) (2022.7.9)\nRequirement already satisfied: ftfy in /local_disk0/.ephemeral_nfs/envs/pythonEnv-9b1d644c-02b5-47b7-853a-504f4a82fd02/lib/python3.11/site-packages (from open_clip_torch) (6.3.1)\nRequirement already satisfied: tqdm in /databricks/python3/lib/python3.11/site-packages (from open_clip_torch) (4.65.0)\nRequirement already satisfied: huggingface-hub in /databricks/python3/lib/python3.11/site-packages (from open_clip_torch) (0.27.1)\nRequirement already satisfied: safetensors in /databricks/python3/lib/python3.11/site-packages (from open_clip_torch) (0.4.2)\nRequirement already satisfied: timm in /local_disk0/.ephemeral_nfs/envs/pythonEnv-9b1d644c-02b5-47b7-853a-504f4a82fd02/lib/python3.11/site-packages (from open_clip_torch) (1.0.15)\nRequirement already satisfied: python-dateutil>=2.8.1 in /databricks/python3/lib/python3.11/site-packages (from pandas) (2.8.2)\nRequirement already satisfied: pytz>=2020.1 in /databricks/python3/lib/python3.11/site-packages (from pandas) (2022.7)\nRequirement already satisfied: six in /usr/lib/python3/dist-packages (from imgaug) (1.16.0)\nRequirement already satisfied: scipy in /databricks/python3/lib/python3.11/site-packages (from imgaug) (1.11.1)\nRequirement already satisfied: Pillow in /databricks/python3/lib/python3.11/site-packages (from imgaug) (9.4.0)\nRequirement already satisfied: matplotlib in /databricks/python3/lib/python3.11/site-packages (from imgaug) (3.7.2)\nRequirement already satisfied: scikit-image>=0.14.2 in /databricks/python3/lib/python3.11/site-packages (from imgaug) (0.20.0)\nRequirement already satisfied: opencv-python in /local_disk0/.ephemeral_nfs/envs/pythonEnv-9b1d644c-02b5-47b7-853a-504f4a82fd02/lib/python3.11/site-packages (from imgaug) (4.11.0.86)\nRequirement already satisfied: imageio in /databricks/python3/lib/python3.11/site-packages (from imgaug) (2.31.1)\nRequirement already satisfied: Shapely in /local_disk0/.ephemeral_nfs/envs/pythonEnv-9b1d644c-02b5-47b7-853a-504f4a82fd02/lib/python3.11/site-packages (from imgaug) (2.1.1)\nRequirement already satisfied: networkx>=2.8 in /databricks/python3/lib/python3.11/site-packages (from scikit-image>=0.14.2->imgaug) (3.1)\nRequirement already satisfied: tifffile>=2019.7.26 in /databricks/python3/lib/python3.11/site-packages (from scikit-image>=0.14.2->imgaug) (2021.7.2)\nRequirement already satisfied: PyWavelets>=1.1.1 in /databricks/python3/lib/python3.11/site-packages (from scikit-image>=0.14.2->imgaug) (1.4.1)\nRequirement already satisfied: packaging>=20.0 in /databricks/python3/lib/python3.11/site-packages (from scikit-image>=0.14.2->imgaug) (23.2)\nRequirement already satisfied: lazy_loader>=0.1 in /databricks/python3/lib/python3.11/site-packages (from scikit-image>=0.14.2->imgaug) (0.2)\nRequirement already satisfied: filelock in /databricks/python3/lib/python3.11/site-packages (from torch>=1.9.0->open_clip_torch) (3.13.4)\nRequirement already satisfied: typing-extensions>=4.8.0 in /databricks/python3/lib/python3.11/site-packages (from torch>=1.9.0->open_clip_torch) (4.10.0)\nRequirement already satisfied: sympy in /databricks/python3/lib/python3.11/site-packages (from torch>=1.9.0->open_clip_torch) (1.11.1)\nRequirement already satisfied: jinja2 in /databricks/python3/lib/python3.11/site-packages (from torch>=1.9.0->open_clip_torch) (3.1.2)\nRequirement already satisfied: fsspec in /databricks/python3/lib/python3.11/site-packages (from torch>=1.9.0->open_clip_torch) (2023.5.0)\nRequirement already satisfied: wcwidth in /databricks/python3/lib/python3.11/site-packages (from ftfy->open_clip_torch) (0.2.5)\nRequirement already satisfied: pyyaml>=5.1 in /databricks/python3/lib/python3.11/site-packages (from huggingface-hub->open_clip_torch) (6.0)\nRequirement already satisfied: requests in /databricks/python3/lib/python3.11/site-packages (from huggingface-hub->open_clip_torch) (2.31.0)\nRequirement already satisfied: contourpy>=1.0.1 in /databricks/python3/lib/python3.11/site-packages (from matplotlib->imgaug) (1.0.5)\nRequirement already satisfied: cycler>=0.10 in /databricks/python3/lib/python3.11/site-packages (from matplotlib->imgaug) (0.11.0)\nRequirement already satisfied: fonttools>=4.22.0 in /databricks/python3/lib/python3.11/site-packages (from matplotlib->imgaug) (4.25.0)\nRequirement already satisfied: kiwisolver>=1.0.1 in /databricks/python3/lib/python3.11/site-packages (from matplotlib->imgaug) (1.4.4)\nRequirement already satisfied: pyparsing<3.1,>=2.3.1 in /databricks/python3/lib/python3.11/site-packages (from matplotlib->imgaug) (3.0.9)\nRequirement already satisfied: MarkupSafe>=2.0 in /databricks/python3/lib/python3.11/site-packages (from jinja2->torch>=1.9.0->open_clip_torch) (2.1.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /databricks/python3/lib/python3.11/site-packages (from requests->huggingface-hub->open_clip_torch) (2.0.4)\nRequirement already satisfied: idna<4,>=2.5 in /databricks/python3/lib/python3.11/site-packages (from requests->huggingface-hub->open_clip_torch) (3.4)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /databricks/python3/lib/python3.11/site-packages (from requests->huggingface-hub->open_clip_torch) (1.26.16)\nRequirement already satisfied: certifi>=2017.4.17 in /databricks/python3/lib/python3.11/site-packages (from requests->huggingface-hub->open_clip_torch) (2023.7.22)\nRequirement already satisfied: mpmath>=0.19 in /databricks/python3/lib/python3.11/site-packages (from sympy->torch>=1.9.0->open_clip_torch) (1.3.0)\n\u001B[43mNote: you may need to restart the kernel using %restart_python or dbutils.library.restartPython() to use updated packages.\u001B[0m\n"
     ]
    }
   ],
   "source": [
    "%pip install opencv-python-headless kaleido open_clip_torch pandas imgaug\n",
    "\n",
    "import cv2\n",
    "import kaleido\n",
    "import numpy as np\n",
    "import open_clip\n",
    "import pandas as pd\n",
    "import time\n",
    "import requests\n",
    "import torch\n",
    "import torchvision.transforms as T\n",
    "from imgaug import augmenters as iaa\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "from functions.gridsearch_modular import *\n",
    "from functions.prep_functions import *\n",
    "\n",
    "class embedder_patch:\n",
    "    \"\"\"\n",
    "    Embeds an image by splitting it into overlapping 224x224 patches, embedding each, and averaging the results.\n",
    "    \"\"\"\n",
    "    def __init__(self, model, device, patch_size=224, normalize=([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])):\n",
    "        self.model = model\n",
    "        self.device = device\n",
    "        self.patch_size = patch_size\n",
    "        self.normalize = normalize\n",
    "\n",
    "    def __str__(self):\n",
    "        return f\"{self.__class__.__name__}_{self.model.__class__.__name__}\"\n",
    "\n",
    "    def ensure_rgb(self, image):\n",
    "        if len(image.shape) == 2:\n",
    "            return np.stack([image]*3, axis=-1)\n",
    "        elif len(image.shape) == 3 and image.shape[2] == 1:\n",
    "            return np.repeat(image, 3, axis=2)\n",
    "        elif len(image.shape) == 3 and image.shape[2] == 3:\n",
    "            return image\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported image shape: {}\".format(image.shape))\n",
    "\n",
    "    def get_patch_positions(self, img_dim):\n",
    "        if img_dim <= self.patch_size:\n",
    "            return [0]\n",
    "        positions = []\n",
    "        stride = self.patch_size\n",
    "        pos = 0\n",
    "        while pos + self.patch_size < img_dim:\n",
    "            positions.append(pos)\n",
    "            pos += stride\n",
    "        positions.append(img_dim - self.patch_size)\n",
    "        return positions\n",
    "\n",
    "    def embed(self, image, normalize=None):\n",
    "        \"\"\"\n",
    "        Returns an embedding for the input image by averaging the embeddings of overlapping patches.\n",
    "\n",
    "        Args:\n",
    "            image: np.array, RGB or grayscale image.\n",
    "            normalize: Optional normalization values to use for the image. If None, uses self.normalize.\n",
    "\n",
    "        Returns:\n",
    "            embedding: np.array, the image embedding.\n",
    "            error: None if no error, else an error message string.\n",
    "        \"\"\"\n",
    "        if normalize is None:\n",
    "            normalize = self.normalize\n",
    "        \n",
    "        try:\n",
    "            transform = T.Compose([\n",
    "                T.ToPILImage(),\n",
    "                T.Resize((self.patch_size, self.patch_size)),\n",
    "                T.ToTensor(),\n",
    "                T.Normalize(mean=normalize[0], std=normalize[1])\n",
    "            ])\n",
    "            image = self.ensure_rgb(image)\n",
    "            h, w, _ = image.shape\n",
    "            x_positions = self.get_patch_positions(w)\n",
    "            y_positions = self.get_patch_positions(h)\n",
    "            patch_embeddings = []\n",
    "\n",
    "            for y in y_positions:\n",
    "                for x in x_positions:\n",
    "                    patch = image[y:y+self.patch_size, x:x+self.patch_size]\n",
    "                    patch = self.ensure_rgb(patch)\n",
    "                    patch_tensor = transform(patch).unsqueeze(0).to(self.device)\n",
    "                    with torch.no_grad():\n",
    "                        if (self.model.__class__.__name__ == \"CLIP\"):\n",
    "                            emb = self.model.encode_image(patch_tensor).cpu().numpy().squeeze()\n",
    "                        else:\n",
    "                            emb = self.model(patch_tensor).cpu().numpy().squeeze()\n",
    "                    patch_embeddings.append(emb)\n",
    "            image_tensor = transform(image).unsqueeze(0).to(self.device)\n",
    "            with torch.no_grad():\n",
    "                if (self.model.__class__.__name__ == \"CLIP\"):\n",
    "                    emb = self.model.encode_image(image_tensor).cpu().numpy().squeeze()\n",
    "                else:\n",
    "                    emb = self.model(image_tensor).cpu().numpy().squeeze()\n",
    "            patch_embeddings.append(emb)\n",
    "\n",
    "            patch_embeddings = np.stack(patch_embeddings, axis=0)\n",
    "            image_embedding = patch_embeddings.mean(axis=0)\n",
    "            return image_embedding, None\n",
    "        except Exception as e:\n",
    "            return None, str(e)   \n",
    "    \n",
    "class KNNBased:\n",
    "    \"\"\"\n",
    "    KNN-based classifier with outlier rejection using distance thresholding and k optimization.\n",
    "    \"\"\"\n",
    "    def __init__(self, distance_metric, k_range, embedder, threshold=None, seed=None):\n",
    "        self.distance_metric = distance_metric\n",
    "        self.k_range = k_range\n",
    "        self.embedder = embedder\n",
    "        self.threshold = threshold\n",
    "        self.seed = seed\n",
    "    \n",
    "    @property\n",
    "    def optimized_param(self):\n",
    "        return [self.k, self.distance_threshold]\n",
    "    \n",
    "    def _prepare_data(self, train): \n",
    "        \"\"\"\n",
    "        Prepares training embeddings, image paths, and folder names from input data.\n",
    "        \"\"\"\n",
    "        all_embeddings_train = []\n",
    "        image_paths_train = []\n",
    "        folder_names_train = []\n",
    "\n",
    "        for folder_name, folder in train.items():\n",
    "            for img_path, img_data in folder.items():\n",
    "                if isinstance(img_data, dict) and \"original\" in img_data:\n",
    "                    all_embeddings_train.append(img_data[\"original\"])\n",
    "                    image_paths_train.append(img_path)\n",
    "                    folder_names_train.append(folder_name)\n",
    "                else:\n",
    "                    all_embeddings_train.append(img_data)\n",
    "                    image_paths_train.append(img_path)\n",
    "                    folder_names_train.append(folder_name)\n",
    "\n",
    "        embeddings_train = np.array(all_embeddings_train)\n",
    "\n",
    "        return embeddings_train, folder_names_train, image_paths_train\n",
    "\n",
    "    def fit(self, train, outliers, predicted_outlier_precentage=0):\n",
    "        \"\"\"\n",
    "        Fits the model by optimizing k and distance threshold using the provided training data and outliers.\n",
    "\n",
    "        Args:\n",
    "            train: Training data in dictionary format.\n",
    "            outliers: Array or list of outlier embeddings.\n",
    "            predicted_outlier_precentage: Expected outlier percentage for threshold tuning.\n",
    "        \"\"\"\n",
    "        embeddings_train, folder_names_train, image_paths_train = self._prepare_data(train)\n",
    "        self.embeddings_train = embeddings_train\n",
    "        self.folder_names_train = folder_names_train\n",
    "        self.image_paths_train = image_paths_train\n",
    "        self.predicted_percentage = predicted_outlier_precentage\n",
    "        self.folder_names = sorted(set(folder_names_train))\n",
    "\n",
    "        if self.seed is not None:\n",
    "            np.random.seed(self.seed)\n",
    "\n",
    "        best_k = None\n",
    "        best_accuracy = float(\"-inf\")\n",
    "\n",
    "        outliers = outliers if outliers is not None else []\n",
    "        outliers = np.array(outliers)\n",
    "\n",
    "        indices_train = np.arange(len(embeddings_train))\n",
    "\n",
    "        n_inlier = len(indices_train)\n",
    "\n",
    "        for k in self.k_range:\n",
    "            predictions = []\n",
    "            outlier_count = 0\n",
    "\n",
    "            for val_idx, val_point in enumerate(embeddings_train):\n",
    "                subset_idx = [idx for idx in indices_train if image_paths_train[idx] != image_paths_train[val_idx]]\n",
    "                base_points = np.array([embeddings_train[idx] for idx in subset_idx])\n",
    "                subset_labels = [folder_names_train[idx] for idx in subset_idx]\n",
    "\n",
    "                if len(base_points) == 0:\n",
    "                    predictions.append(None)\n",
    "                    continue\n",
    "\n",
    "                distances = cdist([val_point], base_points, metric=self.distance_metric).flatten()\n",
    "                nearest_indices = np.argsort(distances)[:k]\n",
    "                neighbor_labels = [subset_labels[idx] for idx in nearest_indices]\n",
    "\n",
    "                label_counts = {}\n",
    "                for label in neighbor_labels:\n",
    "                    label_counts[label] = label_counts.get(label, 0) + 1\n",
    "\n",
    "                sorted_labels = sorted(label_counts.items(), key=lambda x: x[1], reverse=True)\n",
    "                if self.threshold:\n",
    "                    max_label, max_count = sorted_labels[0]\n",
    "                    if (max_count / k) >= self.threshold:\n",
    "                        if len(sorted_labels) > 1:\n",
    "                            second_label, second_count = sorted_labels[1]\n",
    "                            if abs((max_count / k) - (second_count / k)) <= 0.05:\n",
    "                                predictions.append(None)\n",
    "                                outlier_count += 1\n",
    "                            else:\n",
    "                                predictions.append(max_label)\n",
    "                        else:\n",
    "                            predictions.append(max_label)\n",
    "                    else:\n",
    "                        predictions.append(None)\n",
    "                        outlier_count += 1\n",
    "                else:\n",
    "                    if len(sorted_labels) > 1:\n",
    "                        max_label, max_count = sorted_labels[0]\n",
    "                        second_label, second_count = sorted_labels[1]\n",
    "                        if abs((max_count / k) - (second_count / k)) <= 0.05:\n",
    "                            predictions.append(None)\n",
    "                            outlier_count += 1\n",
    "                        else:\n",
    "                            predictions.append(max_label)\n",
    "                    else:\n",
    "                        predictions.append(sorted_labels[0][0])\n",
    "\n",
    "            y_true = [folder_names_train[indices_train[i]] for i in range(len(predictions))]\n",
    "            y_pred = [predictions[i] for i in range(len(predictions))]\n",
    "\n",
    "            inlier_indices = [idx for idx, pred in enumerate(y_pred) if pred is not None]\n",
    "            y_true_inlier = [y_true[i] for i in inlier_indices]\n",
    "            y_pred_inlier = [y_pred[i] for i in inlier_indices]\n",
    "            inlier_accuracy = accuracy_score(y_true_inlier, y_pred_inlier) if y_true_inlier else 0\n",
    "\n",
    "            if self.predicted_percentage > 0:\n",
    "                folder_outlier_indices = [idx for idx, pred in enumerate(y_pred) if pred is None]\n",
    "\n",
    "                n_outlier_expected = len(indices_train) * (self.predicted_percentage//100)\n",
    "                n_outlier_predicted = len(folder_outlier_indices)\n",
    "                \n",
    "                if n_outlier_expected > 0:\n",
    "                    outlier_score = max(0, (1 - abs(n_outlier_predicted - n_outlier_expected) / n_outlier_expected))\n",
    "                else:\n",
    "                    outlier_score = max(0, (1 - n_outlier_predicted))\n",
    "\n",
    "                accuracy = inlier_accuracy + (outlier_score * (self.predicted_percentage//100))\n",
    "            else:\n",
    "                accuracy = inlier_accuracy\n",
    "\n",
    "            if accuracy > best_accuracy:\n",
    "                best_accuracy = accuracy\n",
    "                best_k = k\n",
    "\n",
    "        self.k = best_k\n",
    "\n",
    "        inlier_distances = []\n",
    "        outlier_distances = []\n",
    "\n",
    "        for val_idx, point in enumerate(embeddings_train):\n",
    "            subset_idx = [idx for idx in indices_train if image_paths_train[idx] != image_paths_train[val_idx]]\n",
    "            base_points = np.array([embeddings_train[base_idx] for base_idx in subset_idx])\n",
    "            distances = cdist([point], base_points, metric=self.distance_metric).flatten()\n",
    "            nearest_distances = np.sort(distances)[:self.k]\n",
    "            inlier_distances.append(np.mean(nearest_distances))\n",
    "\n",
    "        for outlier in outliers:\n",
    "            distances = cdist([outlier], embeddings_train, metric=self.distance_metric).flatten()\n",
    "            nearest_distances = np.sort(distances)[:self.k]\n",
    "            outlier_distances.append(np.mean(nearest_distances))\n",
    "\n",
    "        best_threshold = None\n",
    "        best_combined_score = float(\"-inf\")\n",
    "        candidate_thresholds = np.linspace(min(inlier_distances), max(outlier_distances), 100)\n",
    "\n",
    "        for threshold in candidate_thresholds:\n",
    "            inlier_correct = (sum(1 for d in inlier_distances if d <= threshold))/len(inlier_distances)\n",
    "            outlier_correct = (sum(1 for d in outlier_distances if d > threshold))/len(outlier_distances)\n",
    "            combined_score = inlier_correct + outlier_correct\n",
    "\n",
    "            if combined_score > best_combined_score:\n",
    "                best_combined_score = combined_score\n",
    "                best_threshold = threshold\n",
    "\n",
    "        self.distance_threshold = best_threshold\n",
    "\n",
    "    def predict(self, *img_paths):\n",
    "        \"\"\"\n",
    "        Predicts labels for the input image paths using the optimized k value and distance threshold.\n",
    "\n",
    "        Args:\n",
    "            *img_paths: Variable number of image paths.\n",
    "\n",
    "        Returns:\n",
    "            results: List of predicted labels or error messages for each input image.\n",
    "        \"\"\"\n",
    "        if not hasattr(self, \"k\") or self.k is None:\n",
    "            raise ValueError(\"Model has not been fitted or k has not been optimized. Please call fit() first.\")\n",
    "        if not hasattr(self, \"distance_threshold\") or self.distance_threshold is None:\n",
    "            raise ValueError(\"Distance threshold has not been optimized. Please call fit() first.\")\n",
    "        \n",
    "        results = []\n",
    "        for img_path in img_paths:\n",
    "            try:\n",
    "                if urlparse(img_path).scheme in (\"http\", \"https\"):\n",
    "                    img = cv2.imdecode(np.frombuffer(requests.get(img_path).content, np.uint8), 1)\n",
    "                else:\n",
    "                    img = cv2.imread(img_path)\n",
    "                if (img is not None):\n",
    "                    image = img\n",
    "                else:\n",
    "                    raise ValueError(\"returned image is empty or None\")\n",
    "\n",
    "                embedding, error = embedder.embed(image)\n",
    "\n",
    "                if embedding is not None:\n",
    "                    distances = cdist([embedding], self.embeddings_train, metric=self.distance_metric).flatten()\n",
    "                    nearest_indices = np.argsort(distances)[:self.k]\n",
    "                    nearest_distances = distances[nearest_indices]\n",
    "                    neighbor_labels = [self.folder_names_train[idx] for idx in nearest_indices]\n",
    "\n",
    "                    average_distance = np.mean(nearest_distances)\n",
    "\n",
    "                    if average_distance > self.distance_threshold: \n",
    "                        predicted_label = None\n",
    "                    else:\n",
    "                        label_counts = {}\n",
    "                        for label in neighbor_labels:\n",
    "                            label_counts[label] = label_counts.get(label, 0) + 1\n",
    "\n",
    "                        sorted_labels = sorted(label_counts.items(), key=lambda x: x[1], reverse=True)\n",
    "                        if len(sorted_labels) > 0:\n",
    "                            predicted_label = sorted_labels[0][0]\n",
    "                        else:\n",
    "                            predicted_label = None\n",
    "\n",
    "                    results.append(predicted_label)\n",
    "                else:\n",
    "                    results.append(\"error: \" + error)\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing image {os.path.basename(img_path)}: {e}\")\n",
    "                results.append(\"error: \" + e)\n",
    "        return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a54dc8af-8a57-4f03-8332-c611a963410d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/local_disk0/.ephemeral_nfs/envs/pythonEnv-9b1d644c-02b5-47b7-853a-504f4a82fd02/lib/python3.11/site-packages/open_clip/factory.py:388: UserWarning: These pretrained weights were trained with QuickGELU activation but the model config does not have that enabled. Consider using a model config with a \"-quickgelu\" suffix or enable with a flag.\n  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "data_folder = \"/Volumes/rbm_playground/gritty_goat/image_examples_stage_grim/notebook_results/data/\"\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "clip_model, _, preprocess = open_clip.create_model_and_transforms(\"ViT-B-32\", pretrained=\"openai\")\n",
    "model = clip_model.to(device).eval()\n",
    "\n",
    "embedder = embedder_patch(model, device, patch_size=224, normalize=([0.5, 0.5, 0.5], [0.5, 0.5, 0.5]))\n",
    "\n",
    "train = np.load(f\"{data_folder}/embeddings/{embedder}/{embedder}_train.npy\", allow_pickle=True).item()\n",
    "test = np.load(f\"{data_folder}/embeddings/{embedder}/{embedder}_test.npy\", allow_pickle=True).item()\n",
    "train_aug = np.load(f\"{data_folder}/embeddings/{embedder}/{embedder}_train_aug.npy\", allow_pickle=True).item()\n",
    "no_model = np.load(f\"{data_folder}/embeddings/{embedder}/{embedder}_no_model.npy\", allow_pickle=True).item()\n",
    "wrong_upload = np.load(f\"{data_folder}/embeddings/{embedder}/{embedder}_wrong_upload.npy\", allow_pickle=True).item()\n",
    "low_quality = np.load(f\"{data_folder}/embeddings/{embedder}/{embedder}_low_quality.npy\", allow_pickle=True).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7d9ca264-1d3e-4746-8898-e52e6b689c83",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "outliers = []\n",
    "for outlier_dict in [low_quality, no_model, wrong_upload]:\n",
    "    keys = list(outlier_dict[\"outliers\"].keys())\n",
    "    for key in keys:\n",
    "        outliers.append(outlier_dict[\"outliers\"][key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a611f590-e81f-40fd-adb3-7e8192636059",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "model = KNNBased((\"canberra\"), range(1, 50), embedder, threshold=0.65, seed=42)\n",
    "model.fit(train, outliers, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8e256b7b-5a90-4839-9dd2-a7e75431ae34",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_0003 (3.371s)\nNone (1.147s)\nNone (0.448s)\n[None, None, 'model_0002', 'model_0004', 'model_0001', None] (11.256s -> 1.88s/img)\nAverage embedding + prediction time per image: 1.803 seconds\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "pred1 = model.predict(\"/Volumes/rbm_playground/gritty_goat/image_examples_stage_grim/2025_02_13_test_images/model_0003/3035402650 750x.jpg\")\n",
    "duration1 = time.time() - start\n",
    "\n",
    "start = time.time()\n",
    "pred2 = model.predict(\"/Volumes/rbm_playground/gritty_goat/image_examples_stage_grim/2025_05_14/no_model/1e011bd992.png\")\n",
    "duration2 = time.time() - start\n",
    "\n",
    "start = time.time()\n",
    "pred3 = model.predict(\"/Volumes/rbm_playground/gritty_goat/image_examples_stage_grim/2025_05_14/wrong_upload/Schermafbeelding 2025-02-20 160736.png\")\n",
    "duration3 = time.time() - start\n",
    "\n",
    "start = time.time()\n",
    "pred4 = model.predict(\n",
    "    \"/Volumes/rbm_playground/gritty_goat/image_examples_stage_grim/2025_05_14/wrong_upload/image_Nicolas_ann.png\",\n",
    "    \"/Volumes/rbm_playground/gritty_goat/image_examples_stage_grim/2025_05_14/low_quality/001lq19.png\",\n",
    "    \"/Volumes/rbm_playground/gritty_goat/image_examples_stage_grim/2025_02_13_test_images/model_0002/3053605536 011.jpg\",\n",
    "    \"/Volumes/rbm_playground/gritty_goat/image_examples_stage_grim/2025_02_13_test_images/model_0004/YX0975 FEG CS 0010.tif\",\n",
    "    \"https://cdn.ymaws.com/www.thegraphenecouncil.org/resource/resmgr/images/products/product/dec/product_1/product_2/chart_3.jpg\",\n",
    "    \"https://upload.wikimedia.org/wikipedia/commons/thumb/a/a4/Misc_pollen.jpg/1200px-Misc_pollen.jpg\"\n",
    ")\n",
    "duration4 = time.time() - start\n",
    "\n",
    "print(f\"{pred1[0]} ({duration1:.3f}s)\")\n",
    "print(f\"{pred2[0]} ({duration2:.3f}s)\")\n",
    "print(f\"{pred3[0]} ({duration3:.3f}s)\")\n",
    "print(f\"{pred4} ({duration4:.3f}s -> {(duration4/len(pred4)):.2f}s/img)\")\n",
    "\n",
    "total_images = 3 + len(pred4)\n",
    "total_duration = duration1 + duration2 + duration3 + duration4\n",
    "average_duration = total_duration / total_images\n",
    "\n",
    "print(f\"Average embedding + prediction time per image: {average_duration:.3f} seconds\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "103_proofOfConcept",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}